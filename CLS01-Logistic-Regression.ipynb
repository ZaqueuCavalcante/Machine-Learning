{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para a implementação em si.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "# Para comparação com o Scikit Learn.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução\n",
    "\n",
    "A Regresão Logística é uma técnica utilizada para fazer **classificação binária**. Ela permite que um conjunto de ${n}$ variáveis de entrada, sejam contínuas ou discretas, possa ser classificado como pertencente a uma de duas classes (${0}$ ou ${1}$). Para isso, utilizamos a função **sigmóide**, mostrada a seguir:\n",
    "\n",
    "$${y_{prob}} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Onde:\n",
    "- A variável ${y_{prob}}$ indica a **probabilidade** da entrada ${z}$ pertencer à uma das duas classes;\n",
    "- A variável ${z}$ é tal que ${z} = {f(w, b)} = {x_1w_1 + x_2w_2 + ... + x_nw_n} + {b}$, onde:\n",
    "    - ${x}$ é o vetor de dados de entrada, que contém os parâmetros relacionados ao problema;\n",
    "    - ${w}$ é o vetor de pesos, que devem ser ajustados de forma a minimizarem a função de custo;\n",
    "    - ${b}$ é valor do viés, um parâmetro intríseco ao problema;\n",
    "\n",
    "A função sigmóide é implementada a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 42)\n",
    "y_prob = sigmoide(z)\n",
    "\n",
    "plt.figure(dpi=90)\n",
    "plt.scatter(z, y_prob, color='r')\n",
    "plt.title('Função Sigmóide', fontweight=\"bold\") \n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$y_{prob}$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Custo\n",
    "\n",
    "A função de custo da regressão logística, chamada de *Binary Cross-Entropy* ou *Log Loss*, é definida como:\n",
    "\n",
    "$$J(z) = - \\frac{1}{N}\\sum\\limits_{i=1}^{N}{y_{rot(i)}}.log\\left({y_{prob(i)}}\\right) + \\left({1} - {y_{rot(i)}}\\right).log\\left(1-{y_{prob(i)}}\\right)$$\n",
    "\n",
    "Onde:\n",
    "- $J(z)$ é o valor da função de custo a ser minimizada;\n",
    "- $N$ é a quantidade de amostras;\n",
    "- $y_{rot(i)}$ representa a que classe a i-ésima amostra pertence. Ela **rotula** a amostra como ${0}$ ou ${1}$;\n",
    "- $y_{prob(i)}$, como já discutido acima, representa a **probabilidade** i-ésima amostra pertencer a uma das classes;\n",
    "\n",
    "Analisando a função de custo, vamos analisar o que acontece com ela em dois cenários diferentes:\n",
    "- Quando $y_{rot(i)} = 0$, sobra apenas $log(1-{y_{prob(i)}})$ no segundo termo.\n",
    "    - Nesse caso, quando ${y_{prob(i)}}$ tende a $0$, a função de custo também tende. Isso é **consistente**, pois o valor previsto bate com o esperado; \n",
    "    - Já quando ${y_{prob(i)}}$ tende a $1$, a função de custo tende a $-\\infty$, como pode ser visto na figura abaixo;\n",
    "\n",
    "- Quando $y_{rot(i)} = 1$, sobra apenas $log({y_{prob(i)}})$ no primeiro termo.\n",
    "    - Nesse caso, quando ${y_{prob(i)}}$ tende a $1$, a função de custo tende a zero. Isso é **consistente**, pois o valor previsto bate com o esperado;\n",
    "    - Já quando ${y_{prob(i)}}$ tende a $0$, a função de custo tende a $-\\infty$, como pode ser visto na figura abaixo;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_0(y_prob):\n",
    "    return np.log(1 - y_prob)\n",
    "\n",
    "J_0 = J_0(y_prob)\n",
    "\n",
    "plt.figure(figsize=(12,4), dpi=90)\n",
    "plt.subplot(121)\n",
    "plt.scatter(y_prob, J_0, color='r')\n",
    "plt.title('Custo para $\\mathbf{y_{rot(i)} = 0}$', fontweight=\"bold\") \n",
    "plt.xlabel('$y_{prob}$')\n",
    "plt.ylabel('$J_{0}$')\n",
    "plt.grid()\n",
    "\n",
    "def J_1(y_prob):\n",
    "    return np.log(y_prob)\n",
    "\n",
    "J_1 = J_1(y_prob)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(y_prob, J_1, color='r')\n",
    "plt.title('Custo para $\\mathbf{y_{rot(i)} = 1}$', fontweight=\"bold\") \n",
    "plt.xlabel('$y_{prob}$')\n",
    "plt.ylabel('$J_{1}$')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada da função de custo\n",
    "\n",
    "Vamos utilizar a técnica do **Gradiente Descendente** para ajustar o vetor de pesos ${w}$ de forma a minimizar a função de custo $J(z)$. Para isso, precisamos calcular as derivadas de $J(z)$ em relação a ${w}$ e a ${b}$.\n",
    "\n",
    "Inicialmente, vamos manipular a função de custo para que ela passe a depender explicitamente de $z$. Isso facilitará o cálculo das derivadas mais adiante. Você também pode tentar utilizar diretamente a *Regra da Cadeia*, mas os cálculos podem ficar meio grandes, devido à natureza das funções envolvidas.\n",
    "\n",
    "- Calculando $log\\left({y_{prob(i)}}\\right)$:\n",
    "\n",
    "$$log\\left({y_{prob(i)}}\\right) = log\\left(\\frac{1}{1 + e^{-z}}\\right) = log\\left(1\\right) - log\\left({1 + e^{-z}}\\right) = - log\\left({1 + e^{-z}}\\right)$$\n",
    "\n",
    "- Calculando $log\\left(1 - {y_{prob(i)}}\\right)$:\n",
    "\n",
    "$$log\\left(1 - {y_{prob(i)}}\\right) = log\\left(1 - \\frac{1}{1 + e^{-z}}\\right) = log\\left(\\frac{{1 + e^{-z}} - 1}{1 + e^{-z}}\\right) = log\\left(e^{-z}\\right) - log\\left({1 + e^{-z}}\\right) = -z - log\\left({1 + e^{-z}}\\right)$$\n",
    "\n",
    "- Substituindo esses valores na função de custo e simplificando os termos, obtemos que:\n",
    "\n",
    "$$J(z) = - \\frac{1}{N}\\sum\\limits_{i=1}^{N}{y_{rot(i)}}.z - log\\left({1 + e^{z}}\\right)$$\n",
    "\n",
    "- E definimos acima que:\n",
    "\n",
    "$${z} = {f(w, b)} = {x_1w_1 + x_2w_2 + ... + x_nw_n} + {b}$$\n",
    "\n",
    "- Finalmente, calculamos as derivadas parciais:\n",
    "\n",
    "$$\\frac {\\partial {}} {\\partial {}} = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dados\n",
    "\n",
    "Utilizando o dataset tal, que possui como variáveis independetes a *idade* e o *salário* de várias pessoas, vamos tentar classificá-las em uma de duas classes:\n",
    "    \n",
    "- As que **compraram** determinado produto, representadas por $y_{rot(i)} = 1$;\n",
    "- E as que **não compraram**, representadas por $y_{rot(i)} = 0$;,\n",
    "\n",
    "Os dados são importados a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
